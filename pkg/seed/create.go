package seed

import (
	"context"
	"database/sql"
	"fmt"
	"os"
	"path/filepath"
	"sort"
	"strings"

	"github.com/lucasefe/seedup/pkg/pgconn"
)

// CreateOptions configures the seed creation process
type CreateOptions struct {
	DryRun bool
}

// Create creates seed data from a database
// It exports seed data to a single load.sql file
func (s *Seeder) Create(ctx context.Context, dbURL, seedDir, queryFile string, opts CreateOptions) error {
	// Ensure seed directory exists
	if err := os.MkdirAll(seedDir, 0755); err != nil {
		return fmt.Errorf("creating seed directory: %w", err)
	}

	// Open database connection
	db, err := pgconn.Open(dbURL)
	if err != nil {
		return fmt.Errorf("opening database: %w", err)
	}
	defer db.Close()

	// Get all tables in the database
	tables, err := s.getTables(ctx, db)
	if err != nil {
		return fmt.Errorf("getting tables: %w", err)
	}

	// Get tables ordered by FK dependencies
	tableNames := make([]string, len(tables))
	for i, t := range tables {
		tableNames[i] = t.Schema + "." + t.Name
	}
	orderedNames, err := s.getTableOrder(ctx, db, tableNames)
	if err != nil {
		return fmt.Errorf("determining table order: %w", err)
	}

	// Reorder tables based on FK dependencies
	tableMap := make(map[string]tableInfo)
	for _, t := range tables {
		tableMap[t.Schema+"."+t.Name] = t
	}
	orderedTables := make([]tableInfo, 0, len(orderedNames))
	for _, name := range orderedNames {
		if t, ok := tableMap[name]; ok {
			orderedTables = append(orderedTables, t)
		}
	}

	// Extract seed data to a single output file
	outputFile := filepath.Join(seedDir, "load.sql")
	if err := s.extractSeedData(ctx, db, orderedTables, queryFile, outputFile); err != nil {
		return fmt.Errorf("extracting seed data: %w", err)
	}

	if opts.DryRun {
		fmt.Println("Dry run mode - not modifying any files")
		// Remove the generated file in dry run mode
		os.Remove(outputFile)
		return nil
	}

	// Clean old per-table seed files (legacy format)
	oldCSVs, _ := filepath.Glob(filepath.Join(seedDir, "*.csv"))
	for _, csv := range oldCSVs {
		os.Remove(csv)
	}
	// Remove old per-table SQL files but keep dump.sql and load.sql
	oldSQLs, _ := filepath.Glob(filepath.Join(seedDir, "*.*.sql"))
	for _, sqlFile := range oldSQLs {
		os.Remove(sqlFile)
	}

	return nil
}

type tableInfo struct {
	Schema string
	Name   string
}

func (s *Seeder) getTables(ctx context.Context, db *sql.DB) ([]tableInfo, error) {
	rows, err := db.QueryContext(ctx, `
		SELECT schemaname, tablename
		FROM pg_catalog.pg_tables
		WHERE schemaname NOT IN ('information_schema', 'pg_catalog')
		AND schemaname NOT LIKE 'pg_temp%'
		AND tablename <> 'goose_db_version'
		ORDER BY schemaname, tablename
	`)
	if err != nil {
		return nil, fmt.Errorf("querying tables: %w", err)
	}
	defer rows.Close()

	var tables []tableInfo
	for rows.Next() {
		var t tableInfo
		if err := rows.Scan(&t.Schema, &t.Name); err != nil {
			return nil, fmt.Errorf("scanning table info: %w", err)
		}
		tables = append(tables, t)
	}

	if err := rows.Err(); err != nil {
		return nil, fmt.Errorf("iterating tables: %w", err)
	}

	return tables, nil
}

func (s *Seeder) extractSeedData(ctx context.Context, db *sql.DB, tables []tableInfo, queryFile, outputFile string) error {
	// Start a transaction for temp table visibility
	tx, err := db.BeginTx(ctx, nil)
	if err != nil {
		return fmt.Errorf("starting transaction: %w", err)
	}
	defer tx.Rollback()

	// Create temp tables for each real table
	for _, t := range tables {
		tempTable := fmt.Sprintf(`"seed.%s.%s"`, t.Schema, t.Name)
		createSQL := fmt.Sprintf(
			`CREATE TEMP TABLE %s (LIKE %s.%s INCLUDING ALL)`,
			tempTable,
			pgconn.QuoteIdentifier(t.Schema),
			pgconn.QuoteIdentifier(t.Name),
		)
		if _, err := tx.ExecContext(ctx, createSQL); err != nil {
			return fmt.Errorf("creating temp table for %s.%s: %w", t.Schema, t.Name, err)
		}
	}

	// Execute the user's seed query file which populates the temp tables
	if queryFile != "" {
		queryContent, err := os.ReadFile(queryFile)
		if err != nil {
			if os.IsNotExist(err) {
				fmt.Printf("Warning: seed query file '%s' not found, proceeding without custom queries\n", queryFile)
			} else {
				return fmt.Errorf("reading query file: %w", err)
			}
		} else {
			if _, err := tx.ExecContext(ctx, string(queryContent)); err != nil {
				return fmt.Errorf("executing seed query file: %w", err)
			}
		}
	}

	// Build output file content with all tables
	var output strings.Builder
	output.WriteString("-- Seed data generated by seedup\n")
	output.WriteString("-- Tables are ordered by foreign key dependencies\n\n")

	// Generate INSERT statements for each table
	for _, t := range tables {
		insertSQL, rowCount, err := s.generateTableInserts(ctx, tx, t)
		if err != nil {
			return fmt.Errorf("generating inserts for %s.%s: %w", t.Schema, t.Name, err)
		}
		if insertSQL != "" {
			output.WriteString(insertSQL)
			output.WriteString("\n")
		}
		fmt.Printf("Exported %s.%s (%d rows)\n", t.Schema, t.Name, rowCount)
	}

	// Write the consolidated output file
	if err := os.WriteFile(outputFile, []byte(output.String()), 0644); err != nil {
		return fmt.Errorf("writing output file: %w", err)
	}

	// No need to commit - the deferred tx.Rollback() will clean up temp tables
	return nil
}

// generateTableInserts generates a batched INSERT statement for a table.
// Returns the SQL string and row count. Returns empty string if no rows.
func (s *Seeder) generateTableInserts(ctx context.Context, tx *sql.Tx, t tableInfo) (string, int, error) {
	tempTableName := fmt.Sprintf(`pg_temp.seed.%s.%s`, t.Schema, t.Name)
	tempTableQuoted := fmt.Sprintf(`"seed.%s.%s"`, t.Schema, t.Name)

	// Get column info for the temp table
	columns, err := pgconn.GetColumnInfo(ctx, tx, tempTableName)
	if err != nil {
		return "", 0, fmt.Errorf("getting column info: %w", err)
	}

	if len(columns) == 0 {
		// No columns found, skip this table
		fmt.Printf("Warning: no columns found for table %s.%s\n", t.Schema, t.Name)
		return "", 0, nil
	}

	// Query all rows from the temp table
	rows, err := tx.QueryContext(ctx, fmt.Sprintf("SELECT * FROM %s", tempTableQuoted))
	if err != nil {
		return "", 0, fmt.Errorf("querying temp table: %w", err)
	}
	defer rows.Close()

	// Build column names list for INSERT statements
	colNames := make([]string, len(columns))
	for i, col := range columns {
		colNames[i] = pgconn.QuoteIdentifier(col.Name)
	}
	colNamesStr := strings.Join(colNames, ", ")

	// Collect all row values
	var valueRows []string
	for rows.Next() {
		// Create scan destinations
		values := make([]any, len(columns))
		valuePtrs := make([]any, len(columns))
		for i := range values {
			valuePtrs[i] = &values[i]
		}

		if err := rows.Scan(valuePtrs...); err != nil {
			return "", 0, fmt.Errorf("scanning row: %w", err)
		}

		// Serialize values
		serialized := pgconn.SerializeRow(values, columns)
		valueRows = append(valueRows, fmt.Sprintf("    (%s)", strings.Join(serialized, ", ")))
	}

	if err := rows.Err(); err != nil {
		return "", 0, fmt.Errorf("iterating rows: %w", err)
	}

	// Return empty string if no data
	if len(valueRows) == 0 {
		return "", 0, nil
	}

	// Build batched INSERT statement
	var sb strings.Builder
	sb.WriteString(fmt.Sprintf("-- Table: %s.%s\n", t.Schema, t.Name))
	sb.WriteString(fmt.Sprintf("INSERT INTO %s.%s (%s) VALUES\n",
		pgconn.QuoteIdentifier(t.Schema),
		pgconn.QuoteIdentifier(t.Name),
		colNamesStr,
	))
	sb.WriteString(strings.Join(valueRows, ",\n"))
	sb.WriteString(";\n")

	return sb.String(), len(valueRows), nil
}

// getTableOrder returns tables sorted by foreign key dependencies.
// Tables with no dependencies come first, tables that depend on others come later.
func (s *Seeder) getTableOrder(ctx context.Context, db *sql.DB, tables []string) ([]string, error) {
	if len(tables) == 0 {
		return nil, nil
	}

	// Query to get FK dependencies for the given tables
	query := `
		SELECT DISTINCT
			tc.table_schema || '.' || tc.table_name as dependent,
			ccu.table_schema || '.' || ccu.table_name as referenced
		FROM information_schema.table_constraints tc
		JOIN information_schema.constraint_column_usage ccu
			ON tc.constraint_name = ccu.constraint_name
			AND tc.table_schema = ccu.table_schema
		WHERE tc.constraint_type = 'FOREIGN KEY'
	`

	rows, err := db.QueryContext(ctx, query)
	if err != nil {
		// If we can't get dependencies, just return tables in original order
		return tables, nil
	}
	defer rows.Close()

	// Build dependency graph
	deps := make(map[string][]string) // table -> tables it depends on
	for _, table := range tables {
		deps[table] = nil
	}

	for rows.Next() {
		var dependent, referenced string
		if err := rows.Scan(&dependent, &referenced); err != nil {
			continue
		}

		// Only consider dependencies between tables we're importing
		if _, ok := deps[dependent]; ok {
			if _, ok := deps[referenced]; ok {
				deps[dependent] = append(deps[dependent], referenced)
			}
		}
	}

	if err := rows.Err(); err != nil {
		return tables, nil
	}

	// Topological sort using Kahn's algorithm
	inDegree := make(map[string]int)
	for _, table := range tables {
		inDegree[table] = len(deps[table])
	}

	// Start with tables that have no dependencies
	var queue []string
	for _, table := range tables {
		if inDegree[table] == 0 {
			queue = append(queue, table)
		}
	}
	sort.Strings(queue) // Stable ordering

	var result []string
	for len(queue) > 0 {
		// Take first from queue
		table := queue[0]
		queue = queue[1:]
		result = append(result, table)

		// For each table that depends on this one, decrease its in-degree
		for other, dependencies := range deps {
			for _, dep := range dependencies {
				if dep == table {
					inDegree[other]--
					if inDegree[other] == 0 {
						queue = append(queue, other)
						sort.Strings(queue) // Keep stable ordering
					}
				}
			}
		}
	}

	// If we couldn't order all tables (cycle?), append remaining
	if len(result) < len(tables) {
		tableSet := make(map[string]bool)
		for _, t := range result {
			tableSet[t] = true
		}
		for _, t := range tables {
			if !tableSet[t] {
				result = append(result, t)
			}
		}
	}

	return result, nil
}
